{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06ac262-7555-432b-824c-5e5f62579deb",
   "metadata": {},
   "source": [
    "# Training of the Ostia Detector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f36f3a5-be76-460a-a106-20d429ae386f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from math import *\n",
    "from OstiaDetector import OstiaDetector\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72dc9901-2732-40e3-8f71-1852553608a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc1d1c-803a-4e44-b26a-7c2993f7768b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the data and define the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab1a31b-ba31-408e-8909-bea18d531298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directories containing the files\n",
    "image_directory = \"/data/training_data/\"\n",
    "label_directory = \"/data/training_data/\"\n",
    "graph_directory = \"/data/training_data/\"\n",
    "\n",
    "# List all the NIfTI files in the directory\n",
    "image_files = [os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.endswith('img.nii.gz')]\n",
    "label_files = [os.path.join(label_directory, f) for f in os.listdir(label_directory) if f.endswith('label.nii.gz')]\n",
    "graph_files = [os.path.join(graph_directory, f) for f in os.listdir(graph_directory) if f.endswith('graph.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19643cbf-e3d6-4482-ac50-71c69b4ab92e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of dictionnaries with the data, exclude samples with alleged incorrect number of nodes in their ground-truth graphs\n",
    "exclude = [\"102624\", \"27badc\", \"0604cd\", \"26d228\", \"04222e\", \"22cdd3\", \"29db0c\", \"0b9189\", \"1a42d5\", \"1b3c33\", \"2f74d6\", \"108c99\", \n",
    "           \"0cce0b\", \"064c3e\", \"234666\", \"28955b\", \"0a5b04\", \"107165\", \"23c591\", \"0b06d2\",\"089ee1\", \"18beb4\", \"1f594d\"]\n",
    "data = []\n",
    "\n",
    "for i in image_files:\n",
    "    if i.split(image_directory)[1][:6] in exclude:\n",
    "        continue\n",
    "    for l in label_files:\n",
    "        for g in graph_files:\n",
    "            if i.split(image_directory)[1][:6] == l.split(label_directory)[1][:6] == g.split(graph_directory)[1][:6]: # identify the sample\n",
    "                data += [{'image': i, 'label': l, 'graph': g}]\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d287caf-7023-43cc-9bf2-76d17be2fbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training, hold_out = data[:-15], data[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f35dcee-80b9-42db-a960-d10a7c4d9168",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 20\n",
      "batch size: 32 \n",
      "batches per epoch: 162\n",
      "total iterations: 103680\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "batches_per_epoch = len(training)\n",
    "total_iterations = epochs * batch_size * batches_per_epoch\n",
    "print(\"epochs: {}\\nbatch size: {} \\nbatches per epoch: {}\\ntotal iterations: {}\".format(\n",
    "    epochs, batch_size, batches_per_epoch, total_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fda4e12-7143-40bf-ad0f-1217ab9101bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_lrate = 1e-1\n",
    "ostia_model = OstiaDetector().to(device)\n",
    "optimizer = Adam(ostia_model.parameters(), lr=initial_lrate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b14a420-483b-4e19-8aba-14403f859518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    epochs_drop = 10000 / batch_size / batches_per_epoch\n",
    "    drop = 0.1\n",
    "    \n",
    "    lrate = initial_lrate * pow(drop, floor((1 + epoch) / (5 * epochs_drop)))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9e3726-8807-46b5-809d-a2d7d2fff303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f94961-70e4-47e9-9a4a-3c0baaa11d32",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb422ca5-7510-4a65-8023-ed44dd83ef17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing of data, compute reference proximity values for patches centered around ostia points and random points (negative samples)\n",
    "def get_batch(id_elem):\n",
    "    input_data, d_ostia = [], []\n",
    "    \n",
    "    # Constants (from Coronary Artery Centerline Extraction in Cardiac CT Angiography Using a CNN-Based Orientation Classifier)\n",
    "    a = 6\n",
    "    dm_ostium = 16\n",
    "    \n",
    "    # Fraction of patches centered around ostia points\n",
    "    p = 7/10 \n",
    "    \n",
    "    # Get one data sample and retrieve the coronary ostia and the image\n",
    "    sample = data[id_elem]\n",
    "    image, ostia = load_seeds(sample['image'], sample['graph'])\n",
    "    \n",
    "    # List of 0 and 1 at random positions\n",
    "    idx_references = np.random.randint(len(ostia), size=int(p * batch_size))\n",
    "    \n",
    "    # Add samples centered around ostia points\n",
    "    for idx in idx_references:\n",
    "        point = multivariate_normal.rvs(ostia[idx], cov=16) # add gaussian noise to the coordinates of the chosen ostium \n",
    "        patch, bounds = segment_image(image, point)\n",
    "\n",
    "        if patch.shape == (19, 19, 19):\n",
    "            d_ostium = 0\n",
    "            dc_ostium = dm_ostium\n",
    "            \n",
    "            # Get the distance of the patch center to the closest ostium point\n",
    "            for ostium in ostia:\n",
    "                if np.all((ostium >= bounds[0]) & (ostium < bounds[1]), axis=0): # ensure the ostium point is within the patch bounds\n",
    "                    if calculate_distance(point, ostium) < dc_ostium:\n",
    "                        dc_ostium = calculate_distance(point, ostium)\n",
    "\n",
    "            if dc_ostium < dm_ostium: # if the distance is below the defined threshold\n",
    "                d_ostium = np.exp(a * (1 - (dc_ostium/dm_ostium))) - 1\n",
    "                \n",
    "            input_data.append(patch) \n",
    "            d_ostia.append(d_ostium)\n",
    "                    \n",
    "    # Add negative samples\n",
    "    while len(input_data) < batch_size:\n",
    "        point = np.random.randint(image.shape)\n",
    "        patch, _ = segment_image(image, point)\n",
    "        j = 0\n",
    "        \n",
    "        if patch.shape == (19, 19, 19):\n",
    "            # Ensure the random point is not close to any ostium point\n",
    "            for ostium in ostia:\n",
    "                if calculate_distance(point, ostium) > dm_ostium:\n",
    "                    j += 1\n",
    "                    \n",
    "            if j == len(ostia):\n",
    "                input_data.append(patch)\n",
    "                d_ostia.append(0)\n",
    "                    \n",
    "    # Formatting (converting into arrays before converting into tensors reduces the execution time)\n",
    "    input_data = torch.tensor(np.array(input_data), dtype=torch.float32).reshape(-1, 1, 19, 19, 19)\n",
    "    d_ostia = torch.tensor(np.array(d_ostia), dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    return input_data, d_ostia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4015d74d-1eee-4b6b-8fe6-c5573f8d441f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_batch(id_elem):\n",
    "    running_loss = 0.\n",
    "    \n",
    "    X, Y = get_batch(id_elem)\n",
    "\n",
    "    X, Y = X.to(device), Y.to(device)\n",
    "    \n",
    "    # Put gradients to zero for every batch\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Make predictions for this batch\n",
    "    outputs = ostia_model(X)\n",
    "\n",
    "    # Compute the loss and its gradients \n",
    "    loss = loss_fn(Y, outputs)\n",
    "    loss.backward()\n",
    "\n",
    "    # Adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "            \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176664e-d35d-4574-bf2a-1777620f7ec1",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860950c2-b77b-434c-8f91-ee9a72e2c735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 20 loss: 2593.609997558594\n",
      "  batch 40 loss: 1846.0680358886718\n",
      "  batch 60 loss: 1959.745669555664\n",
      "  batch 80 loss: 1373.7342910766602\n",
      "  batch 100 loss: 1464.800830078125\n",
      "\n",
      "  BATCH100 loss: 1847.591764831543\n",
      "\n",
      "  batch 120 loss: 1372.1501510620117\n",
      "  batch 140 loss: 1074.2879638671875\n",
      "  batch 160 loss: 1489.5362518310546\n",
      "TRAIN loss 1489.5362518310546\n",
      "TEST loss 1676.6760782877604\n",
      "EPOCH 2:\n",
      "  batch 20 loss: 1325.3512878417969\n",
      "  batch 40 loss: 1199.1827606201173\n",
      "  batch 60 loss: 1208.3746337890625\n",
      "  batch 80 loss: 1168.408364868164\n",
      "  batch 100 loss: 1214.1961196899415\n",
      "\n",
      "  BATCH100 loss: 1223.1026333618165\n",
      "\n",
      "  batch 120 loss: 1174.4673583984375\n",
      "  batch 140 loss: 1021.461328125\n",
      "  batch 160 loss: 896.9181655883789\n",
      "TRAIN loss 896.9181655883789\n",
      "TEST loss 1411.5084167480468\n",
      "EPOCH 3:\n",
      "  batch 20 loss: 980.699518585205\n",
      "  batch 40 loss: 1428.490330505371\n",
      "  batch 60 loss: 948.400991821289\n",
      "  batch 80 loss: 1194.651988220215\n",
      "  batch 100 loss: 1179.110333251953\n",
      "\n",
      "  BATCH100 loss: 1146.2706324768067\n",
      "\n",
      "  batch 120 loss: 957.4060134887695\n",
      "  batch 140 loss: 854.2831039428711\n",
      "  batch 160 loss: 1008.2647399902344\n",
      "TRAIN loss 1008.2647399902344\n",
      "TEST loss 2543.465372721354\n",
      "EPOCH 4:\n",
      "  batch 20 loss: 1213.9379837036133\n",
      "  batch 40 loss: 1239.7325744628906\n",
      "  batch 60 loss: 1040.0839553833007\n",
      "  batch 80 loss: 1021.18994140625\n",
      "  batch 100 loss: 907.2059036254883\n",
      "\n",
      "  BATCH100 loss: 1084.4300717163087\n",
      "\n",
      "  batch 120 loss: 1046.5773559570312\n",
      "  batch 140 loss: 1052.4754623413087\n",
      "  batch 160 loss: 838.628059387207\n",
      "TRAIN loss 838.628059387207\n",
      "TEST loss 1325.4648071289062\n",
      "EPOCH 5:\n",
      "  batch 20 loss: 1150.8734558105468\n",
      "  batch 40 loss: 1038.084194946289\n",
      "  batch 60 loss: 840.0428985595703\n",
      "  batch 80 loss: 829.0571823120117\n",
      "  batch 100 loss: 920.6597770690918\n",
      "\n",
      "  BATCH100 loss: 955.743501739502\n",
      "\n",
      "  batch 120 loss: 873.6433372497559\n",
      "  batch 140 loss: 952.4919570922851\n",
      "  batch 160 loss: 1047.8223052978515\n",
      "TRAIN loss 1047.8223052978515\n",
      "TEST loss 1303.0870137532552\n",
      "EPOCH 6:\n",
      "  batch 20 loss: 749.3390838623047\n",
      "  batch 40 loss: 1000.693709564209\n",
      "  batch 60 loss: 1042.603533935547\n",
      "  batch 80 loss: 1113.6979652404784\n",
      "  batch 100 loss: 731.873159790039\n",
      "\n",
      "  BATCH100 loss: 927.6414904785156\n",
      "\n",
      "  batch 120 loss: 777.077318572998\n",
      "  batch 140 loss: 940.2341079711914\n",
      "  batch 160 loss: 861.1536712646484\n",
      "TRAIN loss 861.1536712646484\n",
      "TEST loss 1234.7688720703125\n",
      "EPOCH 7:\n",
      "  batch 20 loss: 909.7260032653809\n",
      "  batch 40 loss: 939.1951934814454\n",
      "  batch 60 loss: 854.4557327270508\n",
      "  batch 80 loss: 988.399951171875\n",
      "  batch 100 loss: 957.2082489013671\n",
      "\n",
      "  BATCH100 loss: 929.7970259094238\n",
      "\n",
      "  batch 120 loss: 1164.8160751342773\n",
      "  batch 140 loss: 817.5565338134766\n",
      "  batch 160 loss: 1095.3314987182616\n",
      "TRAIN loss 1095.3314987182616\n",
      "TEST loss 1045.6385091145833\n",
      "EPOCH 8:\n",
      "  batch 20 loss: 991.586637878418\n",
      "  batch 40 loss: 741.6408012390136\n",
      "  batch 60 loss: 891.6213790893555\n",
      "  batch 80 loss: 812.2652442932128\n",
      "  batch 100 loss: 713.0859375\n",
      "\n",
      "  BATCH100 loss: 830.04\n",
      "\n",
      "  batch 120 loss: 941.4841049194335\n",
      "  batch 140 loss: 1023.3833778381347\n",
      "  batch 160 loss: 861.4458381652832\n",
      "TRAIN loss 861.4458381652832\n",
      "TEST loss 1195.5663645426432\n",
      "EPOCH 9:\n",
      "  batch 20 loss: 886.2869171142578\n",
      "  batch 40 loss: 881.8747589111329\n",
      "  batch 60 loss: 1147.7816253662108\n",
      "  batch 80 loss: 762.5235939025879\n",
      "  batch 100 loss: 818.3714012145996\n",
      "\n",
      "  BATCH100 loss: 899.3676593017578\n",
      "\n",
      "  batch 120 loss: 731.0435821533204\n",
      "  batch 140 loss: 787.3323760986328\n",
      "  batch 160 loss: 872.923543548584\n",
      "TRAIN loss 872.923543548584\n",
      "TEST loss 1228.8336568196614\n",
      "EPOCH 10:\n",
      "  batch 20 loss: 1096.282177734375\n",
      "  batch 40 loss: 750.1106414794922\n",
      "  batch 60 loss: 974.6619583129883\n",
      "  batch 80 loss: 914.6841125488281\n",
      "  batch 100 loss: 977.3946838378906\n",
      "\n",
      "  BATCH100 loss: 942.6267147827149\n",
      "\n",
      "  batch 120 loss: 745.7142150878906\n",
      "  batch 140 loss: 873.5249389648437\n",
      "  batch 160 loss: 643.9232574462891\n",
      "TRAIN loss 643.9232574462891\n",
      "TEST loss 939.5534464518229\n",
      "EPOCH 11:\n",
      "  batch 20 loss: 1027.231460571289\n",
      "  batch 40 loss: 806.0821075439453\n",
      "  batch 60 loss: 700.2434005737305\n",
      "  batch 80 loss: 801.3426704406738\n",
      "  batch 100 loss: 627.4541412353516\n",
      "\n",
      "  BATCH100 loss: 792.470756072998\n",
      "\n",
      "  batch 120 loss: 534.4235992431641\n",
      "  batch 140 loss: 726.3633102416992\n",
      "  batch 160 loss: 651.2564872741699\n",
      "TRAIN loss 651.2564872741699\n",
      "TEST loss 779.101259358724\n",
      "EPOCH 12:\n",
      "  batch 20 loss: 743.8601333618165\n",
      "  batch 40 loss: 666.344441986084\n",
      "  batch 60 loss: 701.568830871582\n",
      "  batch 80 loss: 665.780322265625\n",
      "  batch 100 loss: 862.3899711608886\n",
      "\n",
      "  BATCH100 loss: 727.9887399291993\n",
      "\n",
      "  batch 120 loss: 790.2150127410889\n",
      "  batch 140 loss: 730.8612594604492\n",
      "  batch 160 loss: 755.6311706542969\n",
      "TRAIN loss 755.6311706542969\n",
      "TEST loss 1041.61224416097\n",
      "EPOCH 13:\n",
      "  batch 20 loss: 918.4306381225585\n",
      "  batch 40 loss: 812.2587608337402\n",
      "  batch 60 loss: 823.9893287658691\n",
      "  batch 80 loss: 580.5703033447265\n",
      "  batch 100 loss: 928.8042724609375\n",
      "\n",
      "  BATCH100 loss: 812.8106607055664\n",
      "\n",
      "  batch 120 loss: 735.5859573364257\n",
      "  batch 140 loss: 840.3840148925781\n",
      "  batch 160 loss: 755.1274124145508\n",
      "TRAIN loss 755.1274124145508\n",
      "TEST loss 761.4583821614583\n",
      "EPOCH 14:\n",
      "  batch 20 loss: 651.1852516174316\n",
      "  batch 40 loss: 669.2525283813477\n",
      "  batch 60 loss: 694.1474739074707\n",
      "  batch 80 loss: 587.5734405517578\n",
      "  batch 100 loss: 721.2588455200196\n",
      "\n",
      "  BATCH100 loss: 664.6835079956055\n",
      "\n",
      "  batch 120 loss: 584.0352546691895\n",
      "  batch 140 loss: 499.8166030883789\n",
      "  batch 160 loss: 646.0657707214356\n",
      "TRAIN loss 646.0657707214356\n",
      "TEST loss 1013.590707397461\n",
      "EPOCH 15:\n",
      "  batch 20 loss: 788.7804992675781\n",
      "  batch 40 loss: 773.4634414672852\n",
      "  batch 60 loss: 677.9667236328125\n",
      "  batch 80 loss: 906.4573745727539\n",
      "  batch 100 loss: 599.5133064270019\n",
      "\n",
      "  BATCH100 loss: 749.2362690734864\n",
      "\n",
      "  batch 120 loss: 827.4116973876953\n",
      "  batch 140 loss: 529.7521308898926\n",
      "  batch 160 loss: 669.3047653198242\n",
      "TRAIN loss 669.3047653198242\n",
      "TEST loss 593.5010721842448\n",
      "EPOCH 16:\n",
      "  batch 20 loss: 641.0877693176269\n",
      "  batch 40 loss: 634.4231887817383\n",
      "  batch 60 loss: 785.0798210144043\n",
      "  batch 80 loss: 638.0031471252441\n",
      "  batch 100 loss: 754.021826171875\n",
      "\n",
      "  BATCH100 loss: 690.5231504821777\n",
      "\n",
      "  batch 120 loss: 589.7847526550293\n",
      "  batch 140 loss: 650.3331001281738\n",
      "  batch 160 loss: 598.0596435546875\n",
      "TRAIN loss 598.0596435546875\n",
      "TEST loss 973.2800384521485\n",
      "EPOCH 17:\n",
      "  batch 20 loss: 638.9518970489502\n",
      "  batch 40 loss: 452.02586517333987\n",
      "  batch 60 loss: 968.2322143554687\n",
      "  batch 80 loss: 668.9334259033203\n",
      "  batch 100 loss: 623.7746459960938\n",
      "\n",
      "  BATCH100 loss: 670.3836096954345\n",
      "\n",
      "  batch 120 loss: 857.58896484375\n",
      "  batch 140 loss: 622.1104080200196\n",
      "  batch 160 loss: 621.8862586975098\n",
      "TRAIN loss 621.8862586975098\n",
      "TEST loss 869.5957555135091\n",
      "EPOCH 18:\n",
      "  batch 20 loss: 474.54947967529296\n",
      "  batch 40 loss: 756.8238166809082\n",
      "  batch 60 loss: 842.8580749511718\n",
      "  batch 80 loss: 915.1176940917969\n",
      "  batch 100 loss: 555.7640922546386\n",
      "\n",
      "  BATCH100 loss: 709.0226315307617\n",
      "\n",
      "  batch 120 loss: 716.2227951049805\n",
      "  batch 140 loss: 637.0565956115722\n",
      "  batch 160 loss: 642.5478324890137\n",
      "TRAIN loss 642.5478324890137\n",
      "TEST loss 887.4455240885417\n",
      "EPOCH 19:\n",
      "  batch 20 loss: 573.1409774780274\n",
      "  batch 40 loss: 584.7254234313965\n",
      "  batch 60 loss: 868.1740142822266\n",
      "  batch 80 loss: 643.8434860229493\n",
      "  batch 100 loss: 615.1544342041016\n",
      "\n",
      "  BATCH100 loss: 657.0076670837402\n",
      "\n",
      "  batch 120 loss: 850.934757232666\n",
      "  batch 140 loss: 837.3289154052734\n",
      "  batch 160 loss: 714.0707092285156\n",
      "TRAIN loss 714.0707092285156\n",
      "TEST loss 879.0285563151042\n",
      "EPOCH 20:\n",
      "  batch 20 loss: 621.6299781799316\n",
      "  batch 40 loss: 666.8931587219238\n",
      "  batch 60 loss: 921.8612396240235\n",
      "  batch 80 loss: 636.1634918212891\n",
      "  batch 100 loss: 634.613737487793\n",
      "\n",
      "  BATCH100 loss: 696.2323211669922\n",
      "\n",
      "  batch 120 loss: 804.6567993164062\n",
      "  batch 140 loss: 691.230111694336\n",
      "  batch 160 loss: 822.388752746582\n",
      "TRAIN loss 822.388752746582\n",
      "TEST loss 835.0349436442058\n"
     ]
    }
   ],
   "source": [
    "epoch_number = 0\n",
    "train_losses = np.empty(0)\n",
    "validation_losses = np.empty(0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    ostia_model.train(True)\n",
    "    \n",
    "    running_loss = 0.\n",
    "    running_long_loss = 0.\n",
    "    \n",
    "    id_elems = np.arange(len(training))\n",
    "    np.random.shuffle(id_elems)\n",
    "                         \n",
    "    for i, id_elem in enumerate(id_elems):\n",
    "        loss = train_one_batch(id_elem)\n",
    "        running_loss += loss\n",
    "        running_long_loss += loss\n",
    "    \n",
    "        if (i+1) % 20 == 0:\n",
    "            avg_loss = running_loss / 20\n",
    "            print('  batch {} loss: {}'.format(i + 1, avg_loss))\n",
    "            train_losses = np.append(train_losses, avg_loss)\n",
    "            running_loss = 0.\n",
    "            \n",
    "        if (i+1) % 100 == 0:\n",
    "            avg_long_loss = running_long_loss / 100\n",
    "            print('\\n  BATCH100 loss: {}\\n'.format(avg_long_loss))\n",
    "            running_long_loss = 0.\n",
    "            \n",
    "    new_lrate = step_decay(epoch_number)\n",
    "    optimizer.param_groups[0]['lr'] = new_lrate\n",
    "    \n",
    "    # We don't need gradients on to do reporting and testing\n",
    "    ostia_model.train(False)\n",
    "    \n",
    "    print('TRAIN loss {}'.format(avg_loss))\n",
    "    \n",
    "    running_vloss = 0.\n",
    "    id_elems = np.arange(len(training), len(data))\n",
    "                         \n",
    "    for id_elem in id_elems:\n",
    "        vloss = train_one_batch(id_elem)\n",
    "        running_vloss += vloss\n",
    "    \n",
    "    avg_vloss = running_vloss / len(id_elems)\n",
    "    \n",
    "    validation_losses = np.append(validation_losses, avg_vloss)\n",
    "    \n",
    "    print('TEST loss {}'.format(avg_vloss))\n",
    "    \n",
    "    epoch_number += 1   \n",
    "    model_path = './models/ostia_model_{}_{}'.format(timestamp, epoch_number)\n",
    "    torch.save(ostia_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0875606-a62c-4f9c-8fa9-5b27037206a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savez(\"losses.npz\", arr1=train_losses, arr2=validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054b78b-0ab0-4da5-9004-4a3f3e9a6d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Exploration Tutorial",
   "language": "python",
   "name": "exploration-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
